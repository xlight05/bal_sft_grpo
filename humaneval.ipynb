{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install transformers==4.51.3\n",
    "    !pip install --no-deps unsloth\n",
    "    !pip install nltk rouge-score numpy requests\n",
    "    !pip install ballerina-platform-codebleu\n",
    "    !pip install tree-sitter-ballerina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a pragmatic Ballerina programmer who enjoys test driven development. Given the following question, write a Ballerina function to complete the task and then write the the unit tests to validate the function.\n",
    "\n",
    "1. Make the code simple and easy to understand.\n",
    "2. Try to limit library usage to the standard library. Be careful with your types, and try to limit yourself to the basic built in types and standard library functions.\n",
    "3. Before you start writing the function you can think through how to solve the problem and perform reasoning in the comments above the function.\n",
    "4. Then write unit tests for the function you defined. Make sure to write at least 4 assertions to test the function. The tests should be a simple.\n",
    "\n",
    "Strictly follow the following output format for each response: Make sure to include code inside <CODE> and <TESTS> blocks.\n",
    "\n",
    "# Overview\n",
    "Brief overview about the solution.\n",
    "\n",
    "<CODE>\n",
    "```ballerina\n",
    "// Reasoning goes here\n",
    "// and can be multi-line\n",
    "function add(int a, int b) returns int {\n",
    "    return a + b;\n",
    "}\n",
    "```\n",
    "</CODE>\n",
    "\n",
    "<TESTS>\n",
    "```ballerina\n",
    "import ballerina/test;\n",
    "\n",
    "@test:Config { }\n",
    "function testAssertEquals() {\n",
    "    int addResult = add(40, 2);\n",
    "    test:assertEquals(addResult, 42);\n",
    "\n",
    "    addResult = add(0, 0);\n",
    "    test:assertEquals(addResult, 0);\n",
    "\n",
    "    addResult = add(-1, 1);\n",
    "    test:assertEquals(addResult, 0);\n",
    "\n",
    "    addResult = add(-5, -5);\n",
    "    test:assertEquals(addResult, -10);\n",
    "}\n",
    "```\n",
    "</TESTS>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ballerina Base Model Evaluator\n",
    "Evaluates the base Qwen2.5-Coder-7B-Instruct model on Ballerina code generation tasks\n",
    "before fine-tuning to establish baseline metrics.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Install required packages\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"nltk\"], check=True)\n",
    "    import nltk\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"rouge-score\"], check=True)\n",
    "    from rouge_score import rouge_scorer\n",
    "\n",
    "try:\n",
    "    from codebleu import calc_codebleu, AVAILABLE_LANGS\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"ballerina-platform-codebleu\"], check=True)\n",
    "    subprocess.run([\"pip\", \"install\", \"tree-sitter-ballerina\"], check=True)\n",
    "    from codebleu import calc_codebleu, AVAILABLE_LANGS\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    from unsloth.chat_templates import get_chat_template\n",
    "except ImportError:\n",
    "    subprocess.run([\"pip\", \"install\", \"unsloth[colab-new]\"], check=True)\n",
    "    from unsloth import FastLanguageModel\n",
    "    from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "import difflib\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "class BallerinaManager:\n",
    "    def __init__(self, project_path: str = \".\"):\n",
    "        self.project_path = project_path\n",
    "\n",
    "    def get_build_status(self) -> Dict[str, any]:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"bal\", \"build\", \"--offline\"],\n",
    "                cwd=self.project_path,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": result.returncode == 0,\n",
    "                \"return_code\": result.returncode,\n",
    "                \"stdout\": result.stdout,\n",
    "                \"stderr\": result.stderr,\n",
    "                \"compilation_errors\": self._extract_compilation_errors(result.stdout + result.stderr)\n",
    "            }\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"Build process timed out\",\n",
    "                \"compilation_errors\": [\"Build process timed out after 60 seconds\"]\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"bal command not found\",\n",
    "                \"compilation_errors\": [\"Ballerina CLI not found. Please ensure Ballerina is installed and in PATH\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": str(e),\n",
    "                \"compilation_errors\": [f\"Unexpected error: {str(e)}\"]\n",
    "            }\n",
    "\n",
    "    def get_test_status(self) -> Dict[str, any]:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"bal\", \"test\", \"--offline\"],\n",
    "                cwd=self.project_path,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=120\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": result.returncode == 0,\n",
    "                \"return_code\": result.returncode,\n",
    "                \"stdout\": result.stdout,\n",
    "                \"stderr\": result.stderr,\n",
    "                \"test_results\": self._extract_test_results(result.stdout + result.stderr),\n",
    "                \"compilation_errors\": self._extract_compilation_errors(result.stdout + result.stderr)\n",
    "            }\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"Test process timed out\",\n",
    "                \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                \"compilation_errors\": [\"Test process timed out after 120 seconds\"]\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"bal command not found\",\n",
    "                \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                \"compilation_errors\": [\"Ballerina CLI not found. Please ensure Ballerina is installed and in PATH\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": str(e),\n",
    "                \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                \"compilation_errors\": [f\"Unexpected error: {str(e)}\"]\n",
    "            }\n",
    "\n",
    "    def _extract_compilation_errors(self, output: str) -> List[str]:\n",
    "        errors = []\n",
    "        lines = output.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(keyword in line.lower() for keyword in ['error:', 'compilation error', 'build failed']):\n",
    "                errors.append(line)\n",
    "            elif line.startswith('ERROR') or 'error occurred' in line.lower():\n",
    "                errors.append(line)\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _extract_test_results(self, output: str) -> Dict[str, int]:\n",
    "        results = {\"passed\": 0, \"failed\": 0, \"total\": 0}\n",
    "        lines = output.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Look for Ballerina test output format: \"X passing\", \"Y failing\", \"Z skipped\"\n",
    "            if 'passing' in line:\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if part == 'passing' and i > 0:\n",
    "                            results[\"passed\"] = int(parts[i-1])\n",
    "                            break\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "            elif 'failing' in line:\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if part == 'failing' and i > 0:\n",
    "                            results[\"failed\"] = int(parts[i-1])\n",
    "                            break\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "\n",
    "        results[\"total\"] = results[\"passed\"] + results[\"failed\"]\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "from uuid import uuid4\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "Define functions for setting up and testing Ballerina projects.\n",
    "\"\"\"\n",
    "\n",
    "def create_ballerina_toml(package_name: str) -> str:\n",
    "    return f\"\"\"[package]\n",
    "org = \"test\"\n",
    "name = \"test_project\"\n",
    "version = \"0.1.0\"\n",
    "distribution = \"2201.12.7\"\n",
    "\n",
    "[build-options]\n",
    "observabilityIncluded = false\n",
    "\"\"\"\n",
    "\n",
    "def create_main_bal(main_content: str) -> str:\n",
    "    return f\"\"\"{main_content}\"\"\"\n",
    "\n",
    "def create_test_bal(test_content: str) -> str:\n",
    "    return f\"\"\"{test_content}\"\"\"\n",
    "\n",
    "def setup_build_ballerina(main_content: str, test_content: str) -> dict:\n",
    "    \"\"\"Set up temporary Ballerina project and run build with error handling\"\"\"\n",
    "    try:\n",
    "        # Create temporary directory with random UUID suffix\n",
    "        package_name = f\"test-project-{str(uuid4())[:8]}\"\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            project_dir = Path(temp_dir) / package_name\n",
    "            project_dir.mkdir()\n",
    "            tests_dir = project_dir / \"tests\"\n",
    "            tests_dir.mkdir()\n",
    "\n",
    "            # Write project files\n",
    "            (project_dir / \"Ballerina.toml\").write_text(create_ballerina_toml(package_name))\n",
    "            (project_dir / \"main.bal\").write_text(create_main_bal(main_content))\n",
    "            (tests_dir / \"test.bal\").write_text(create_test_bal(test_content))\n",
    "\n",
    "            # Use BallerinaManager to get build status\n",
    "            ballerina_manager = BallerinaManager(str(project_dir))\n",
    "            build_result = ballerina_manager.get_build_status()\n",
    "\n",
    "            return {\n",
    "                \"build_passed\": build_result[\"success\"],\n",
    "                \"build_stderr\": build_result[\"stderr\"],\n",
    "                \"compilation_errors\": build_result[\"compilation_errors\"],\n",
    "                \"package_name\": package_name\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Ballerina project: {e}\")\n",
    "        return {\n",
    "            \"build_passed\": False,\n",
    "            \"build_stderr\": f\"Project setup error: {e}\",\n",
    "            \"compilation_errors\": [f\"Project setup error: {e}\"],\n",
    "            \"package_name\": \"unknown\"\n",
    "        }\n",
    "\n",
    "def setup_build_test_ballerina(main_content: str, test_content: str) -> dict:\n",
    "    \"\"\"Set up temporary Ballerina project with tests and run build and test with error handling\"\"\"\n",
    "    try:\n",
    "        # Create temporary directory with random UUID suffix\n",
    "        package_name = f\"test-project-{str(uuid4())[:8]}\"\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            project_dir = Path(temp_dir) / package_name\n",
    "            project_dir.mkdir()\n",
    "            tests_dir = project_dir / \"tests\"\n",
    "            tests_dir.mkdir()\n",
    "\n",
    "            # Write project files\n",
    "            (project_dir / \"Ballerina.toml\").write_text(create_ballerina_toml(package_name))\n",
    "            (project_dir / \"main.bal\").write_text(create_main_bal(main_content))\n",
    "            (tests_dir / \"test.bal\").write_text(create_test_bal(test_content))\n",
    "\n",
    "            # Use BallerinaManager to get build and test status\n",
    "            ballerina_manager = BallerinaManager(str(project_dir))\n",
    "\n",
    "            # Get build status first\n",
    "            build_result = ballerina_manager.get_build_status()\n",
    "\n",
    "            # Get test status only if build succeeds\n",
    "            if build_result[\"success\"]:\n",
    "                test_result = ballerina_manager.get_test_status()\n",
    "            else:\n",
    "                test_result = {\n",
    "                    \"success\": False,\n",
    "                    \"stdout\": \"\",\n",
    "                    \"stderr\": \"Build failed, skipping tests\",\n",
    "                    \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                    \"compilation_errors\": []\n",
    "                }\n",
    "\n",
    "            return {\n",
    "                \"build_passed\": build_result[\"success\"],\n",
    "                \"build_stderr\": build_result[\"stderr\"],\n",
    "                \"build_compilation_errors\": build_result[\"compilation_errors\"],\n",
    "                \"test_passed\": test_result[\"success\"],\n",
    "                \"test_stderr\": test_result[\"stderr\"],\n",
    "                \"test_results\": test_result[\"test_results\"],\n",
    "                \"test_compilation_errors\": test_result[\"compilation_errors\"],\n",
    "                \"package_name\": package_name\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Ballerina project with tests: {e}\")\n",
    "        return {\n",
    "            \"build_passed\": False,\n",
    "            \"build_stderr\": f\"Project setup error: {e}\",\n",
    "            \"build_compilation_errors\": [f\"Project setup error: {e}\"],\n",
    "            \"test_passed\": False,\n",
    "            \"test_stderr\": f\"Project setup error: {e}\",\n",
    "            \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "            \"test_compilation_errors\": [f\"Project setup error: {e}\"],\n",
    "            \"package_name\": \"unknown\"\n",
    "        }\n",
    "\n",
    "print(\"âœ… Ballerina project setup functions defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code_start = \"<CODE>\"\n",
    "code_end = \"</CODE>\"\n",
    "test_start = \"<TESTS>\"\n",
    "test_end = \"</TESTS>\"\n",
    "\n",
    "def extract_ballerina_code(response: str) -> str:\n",
    "    \"\"\"Extract Ballerina code from response - extracts content inside ```ballerina blocks within <CODE> tags\"\"\"\n",
    "    # Extract everything between <CODE> and </CODE>\n",
    "    pattern = rf\"{re.escape(code_start)}(.*?){re.escape(code_end)}\"\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        # Now extract content from ```ballerina code block\n",
    "        ballerina_pattern = r\"```ballerina\\s*(.*?)\\s*```\"\n",
    "        ballerina_match = re.search(ballerina_pattern, content, re.DOTALL)\n",
    "        if ballerina_match:\n",
    "            return ballerina_match.group(1).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def extract_ballerina_tests(response: str) -> str:\n",
    "    \"\"\"Extract Ballerina tests from response - extracts content inside ```ballerina blocks within <TESTS> tags\"\"\"\n",
    "    # Extract everything between <TESTS> and </TESTS>\n",
    "    pattern = rf\"{re.escape(test_start)}(.*?){re.escape(test_end)}\"\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        # Now extract content from ```ballerina code block\n",
    "        ballerina_pattern = r\"```ballerina\\s*(.*?)\\s*```\"\n",
    "        ballerina_match = re.search(ballerina_pattern, content, re.DOTALL)\n",
    "        if ballerina_match:\n",
    "            return ballerina_match.group(1).strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def exact_ballerina_main_code(content: str) -> str:\n",
    "    \"\"\"Extract main Ballerina code content\"\"\"\n",
    "    return extract_ballerina_code(content)\n",
    "\n",
    "def exact_ballerina_test_code(content: str) -> str:\n",
    "    \"\"\"Extract test Ballerina code content\"\"\"\n",
    "    return extract_ballerina_tests(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BallerinaBaseEvaluator:\n",
    "    def __init__(self, model_name=\"unsloth/Qwen2.5-Coder-7B-Instruct\", max_seq_length=2048, debug_dir=None):\n",
    "        self.model_name = model_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.smoothing_function = SmoothingFunction().method1\n",
    "        self.ballerina_manager = BallerinaManager()\n",
    "        \n",
    "        # Set up debug directory for persistence\n",
    "        if debug_dir is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            debug_dir = f\"debug_outputs/{timestamp}\"\n",
    "        self.debug_dir = Path(debug_dir)\n",
    "        self.debug_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load base model\n",
    "        print(f\"Loading base model: {model_name}\")\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "\n",
    "        # Set up chat template\n",
    "        self.tokenizer = get_chat_template(\n",
    "            self.tokenizer,\n",
    "            chat_template=\"qwen-2.5\",\n",
    "        )\n",
    "\n",
    "        # Enable inference mode\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "        print(\"Base model loaded successfully!\")\n",
    "\n",
    "    def _persist_evaluation_data(self, metric_type: str, problem_idx: int, prompt: str, reference: str, \n",
    "                               candidate_idx: int, raw_response: str, extracted_code: str, \n",
    "                               extracted_tests: str, result: Dict[str, Any], k_value: int):\n",
    "        \"\"\"Persist evaluation data for debugging purposes\"\"\"\n",
    "        try:\n",
    "            # Create directory structure\n",
    "            metric_dir = self.debug_dir / f\"{metric_type}_at_{k_value}\"\n",
    "            problem_dir = metric_dir / f\"problem_{problem_idx+1:03d}\"\n",
    "            candidate_dir = problem_dir / f\"candidate_{candidate_idx+1:03d}\"\n",
    "            candidate_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Save prompt and reference (only once per problem)\n",
    "            prompt_file = problem_dir / \"prompt.txt\"\n",
    "            if not prompt_file.exists():\n",
    "                prompt_file.write_text(prompt, encoding='utf-8')\n",
    "                \n",
    "            reference_file = problem_dir / \"reference_answer.txt\"\n",
    "            if not reference_file.exists():\n",
    "                reference_file.write_text(reference, encoding='utf-8')\n",
    "            \n",
    "            # Save candidate-specific data\n",
    "            (candidate_dir / \"raw_response.txt\").write_text(raw_response, encoding='utf-8')\n",
    "            \n",
    "            if extracted_code:\n",
    "                (candidate_dir / \"extracted_code.bal\").write_text(extracted_code, encoding='utf-8')\n",
    "            \n",
    "            if extracted_tests:\n",
    "                (candidate_dir / \"extracted_tests.bal\").write_text(extracted_tests, encoding='utf-8')\n",
    "            \n",
    "            # Save results based on metric type\n",
    "            if metric_type == \"pass\":\n",
    "                result_file = candidate_dir / \"test_result.json\"\n",
    "            else:\n",
    "                result_file = candidate_dir / \"compilation_result.json\"\n",
    "            \n",
    "            with open(result_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to persist debug data: {e}\")\n",
    "\n",
    "    def _save_evaluation_metadata(self, dataset_size: int, num_samples: int, max_tokens: int):\n",
    "        \"\"\"Save evaluation metadata\"\"\"\n",
    "        try:\n",
    "            metadata = {\n",
    "                \"model_name\": self.model_name,\n",
    "                \"evaluation_timestamp\": datetime.now().isoformat(),\n",
    "                \"dataset_size\": dataset_size,\n",
    "                \"num_samples_per_prompt\": num_samples,\n",
    "                \"max_new_tokens\": max_tokens,\n",
    "                \"system_prompt\": SYSTEM_PROMPT\n",
    "            }\n",
    "            \n",
    "            metadata_file = self.debug_dir / \"metadata.json\"\n",
    "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "                \n",
    "            print(f\"Debug data will be saved to: {self.debug_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to save metadata: {e}\")\n",
    "\n",
    "    def preprocess_code(self, code: str) -> str:\n",
    "        \"\"\"Normalize code for comparison\"\"\"\n",
    "        lines = [line.strip() for line in code.split('\\n') if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def tokenize_for_bleu(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text for BLEU score calculation\"\"\"\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        return tokens\n",
    "\n",
    "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Calculate BLEU score between reference and candidate code\"\"\"\n",
    "        ref_tokens = [self.tokenize_for_bleu(reference)]\n",
    "        cand_tokens = self.tokenize_for_bleu(candidate)\n",
    "\n",
    "        if not cand_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        return sentence_bleu(ref_tokens, cand_tokens, smoothing_function=self.smoothing_function)\n",
    "\n",
    "    def calculate_codebleu_score(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate CodeBLEU score between reference and candidate code\"\"\"\n",
    "        try:\n",
    "            if not reference.strip() or not candidate.strip():\n",
    "                return {\n",
    "                    'codebleu': 0.0,\n",
    "                    'ngram_match_score': 0.0,\n",
    "                    'weighted_ngram_match_score': 0.0,\n",
    "                    'syntax_match_score': 0.0,\n",
    "                    'dataflow_match_score': 0.0\n",
    "                }\n",
    "            \n",
    "            result = calc_codebleu(\n",
    "                [reference],\n",
    "                [candidate],\n",
    "                lang=\"ballerina\",\n",
    "                weights=(0.25, 0.25, 0.25, 0.25)\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'codebleu': result.get('codebleu', 0.0),\n",
    "                'ngram_match_score': result.get('ngram_match_score', 0.0),\n",
    "                'weighted_ngram_match_score': result.get('weighted_ngram_match_score', 0.0),\n",
    "                'syntax_match_score': result.get('syntax_match_score', 0.0),\n",
    "                'dataflow_match_score': result.get('dataflow_match_score', 0.0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: CodeBLEU calculation failed: {e}\")\n",
    "            return {\n",
    "                'codebleu': 0.0,\n",
    "                'ngram_match_score': 0.0,\n",
    "                'weighted_ngram_match_score': 0.0,\n",
    "                'syntax_match_score': 0.0,\n",
    "                'dataflow_match_score': 0.0\n",
    "            }\n",
    "\n",
    "    def calculate_rouge_scores(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ROUGE scores\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, candidate)\n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        }\n",
    "\n",
    "    def calculate_exact_match(self, reference: str, candidate: str) -> bool:\n",
    "        \"\"\"Calculate exact match after normalization\"\"\"\n",
    "        ref_normalized = self.preprocess_code(reference)\n",
    "        cand_normalized = self.preprocess_code(candidate)\n",
    "        return ref_normalized == cand_normalized\n",
    "\n",
    "    def calculate_edit_distance(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Calculate normalized edit distance\"\"\"\n",
    "        ref_normalized = self.preprocess_code(reference)\n",
    "        cand_normalized = self.preprocess_code(candidate)\n",
    "\n",
    "        if not ref_normalized and not cand_normalized:\n",
    "            return 0.0\n",
    "\n",
    "        max_len = max(len(ref_normalized), len(cand_normalized))\n",
    "        if max_len == 0:\n",
    "            return 0.0\n",
    "\n",
    "        edit_dist = len(list(difflib.unified_diff(ref_normalized.split(), cand_normalized.split())))\n",
    "        return 1.0 - (edit_dist / max_len)\n",
    "\n",
    "    def check_ballerina_syntax(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check if Ballerina code compiles and extract syntax information using BallerinaManager\"\"\"\n",
    "        try:\n",
    "            # Use the setup_build_ballerina function to create a proper project and test compilation\n",
    "            build_result = setup_build_ballerina(code, \"\")\n",
    "\n",
    "            compilation_success = build_result[\"build_passed\"]\n",
    "            error_message = build_result[\"build_stderr\"]\n",
    "\n",
    "            # If there are compilation errors, use them as the error message\n",
    "            if build_result[\"compilation_errors\"]:\n",
    "                error_message = \"; \".join(build_result[\"compilation_errors\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback in case of any unexpected errors\n",
    "            compilation_success = False\n",
    "            error_message = f\"Error during compilation check: {str(e)}\"\n",
    "\n",
    "        return {\n",
    "            'compiles': compilation_success,\n",
    "            'error_message': error_message,\n",
    "            'has_function': 'function' in code.lower(),\n",
    "            'has_service': 'service' in code.lower(),\n",
    "            'has_import': 'import' in code.lower(),\n",
    "            'has_return': 'return' in code.lower()\n",
    "        }\n",
    "\n",
    "    def calculate_compile_at_k(self, test_cases: List[str], candidates: List[List[str]], prompts: List[str], k: int = 1) -> float:\n",
    "        \"\"\"Calculate compile@k metric for compilation success only\"\"\"\n",
    "        if len(test_cases) != len(candidates):\n",
    "            raise ValueError(\"Number of test cases must match number of candidate lists\")\n",
    "\n",
    "        total_problems = len(test_cases)\n",
    "        passed_problems = 0\n",
    "\n",
    "        for i, (test_case, cands, prompt) in enumerate(zip(test_cases, candidates, prompts)):\n",
    "            # Take first k candidates\n",
    "            k_candidates = cands[:k]\n",
    "\n",
    "            # Check if any of the k candidates compile successfully\n",
    "            any_compiles = False\n",
    "            for j, cand in enumerate(k_candidates):\n",
    "                # Extract Ballerina code from LLM response\n",
    "                extracted_code = exact_ballerina_main_code(cand)\n",
    "                if extracted_code:  # Only check if we extracted valid code\n",
    "                    syntax_check = self.check_ballerina_syntax(extracted_code)\n",
    "                    \n",
    "                    # Persist debug data\n",
    "                    self._persist_evaluation_data(\n",
    "                        metric_type=\"compile\",\n",
    "                        problem_idx=i,\n",
    "                        prompt=prompt,\n",
    "                        reference=test_case,  # Use test case as reference for debug\n",
    "                        candidate_idx=j,\n",
    "                        raw_response=cand,\n",
    "                        extracted_code=extracted_code,\n",
    "                        extracted_tests=\"\",\n",
    "                        result=syntax_check,\n",
    "                        k_value=k\n",
    "                    )\n",
    "                    \n",
    "                    if syntax_check['compiles']:\n",
    "                        any_compiles = True\n",
    "                        print(f\"  Problem {i+1}: Candidate {j+1} compiled successfully!\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"  Problem {i+1}: Candidate {j+1} failed to compile\")\n",
    "                else:\n",
    "                    # Persist data for failed extractions too\n",
    "                    no_code_result = {\n",
    "                        'compiles': False,\n",
    "                        'error_message': 'No valid Ballerina code found in response',\n",
    "                        'has_function': False,\n",
    "                        'has_service': False,\n",
    "                        'has_import': False,\n",
    "                        'has_return': False\n",
    "                    }\n",
    "                    self._persist_evaluation_data(\n",
    "                        metric_type=\"compile\",\n",
    "                        problem_idx=i,\n",
    "                        prompt=prompt,\n",
    "                        reference=test_case,  # Use test case as reference for debug\n",
    "                        candidate_idx=j,\n",
    "                        raw_response=cand,\n",
    "                        extracted_code=\"\",\n",
    "                        extracted_tests=\"\",\n",
    "                        result=no_code_result,\n",
    "                        k_value=k\n",
    "                    )\n",
    "                    \n",
    "            if any_compiles:\n",
    "                passed_problems += 1\n",
    "\n",
    "        print(f\"Compile@{k}: {passed_problems}/{total_problems} problems compiled\")\n",
    "        return passed_problems / total_problems if total_problems > 0 else 0.0\n",
    "\n",
    "    def calculate_pass_at_k(self, test_cases: List[str], candidates: List[List[str]], prompts: List[str], k: int = 1) -> float:\n",
    "        \"\"\"Calculate pass@k metric for unit test success\"\"\"\n",
    "        if len(test_cases) != len(candidates):\n",
    "            raise ValueError(\"Number of test cases must match number of candidate lists\")\n",
    "\n",
    "        total_problems = len(test_cases)\n",
    "        passed_problems = 0\n",
    "        skipped_problems = 0\n",
    "\n",
    "        for i, (test_case, cands, prompt) in enumerate(zip(test_cases, candidates, prompts)):\n",
    "            # Take first k candidates\n",
    "            k_candidates = cands[:k]\n",
    "            \n",
    "            # Check if any of the k candidates pass the unit tests\n",
    "            any_passes = False\n",
    "            for j, cand in enumerate(k_candidates):\n",
    "                # Extract generated code from LLM response\n",
    "                extracted_code = exact_ballerina_main_code(cand)\n",
    "                \n",
    "                if extracted_code:  # Only check if we extracted valid code\n",
    "                    # Test the generated code against the provided test cases\n",
    "                    test_result = setup_build_test_ballerina(extracted_code, test_case)\n",
    "                    \n",
    "                    # Persist debug data\n",
    "                    self._persist_evaluation_data(\n",
    "                        metric_type=\"pass\",\n",
    "                        problem_idx=i,\n",
    "                        prompt=prompt,\n",
    "                        reference=test_case,  # Use test case as reference for debug\n",
    "                        candidate_idx=j,\n",
    "                        raw_response=cand,\n",
    "                        extracted_code=extracted_code,\n",
    "                        extracted_tests=test_case,\n",
    "                        result=test_result,\n",
    "                        k_value=k\n",
    "                    )\n",
    "                    \n",
    "                    if test_result[\"build_passed\"] and test_result[\"test_passed\"] and test_result[\"test_results\"][\"total\"] > 0:\n",
    "                        # All tests must pass\n",
    "                        if test_result[\"test_results\"][\"failed\"] == 0 and test_result[\"test_results\"][\"passed\"] > 0:\n",
    "                            any_passes = True\n",
    "                            print(f\"  Problem {i+1}: Candidate {j+1} passed all {test_result['test_results']['passed']} tests!\")\n",
    "                            break\n",
    "                    else:\n",
    "                        if not test_result[\"build_passed\"]:\n",
    "                            print(f\"  Problem {i+1}: Candidate {j+1} failed to compile\")\n",
    "                        elif not test_result[\"test_passed\"]:\n",
    "                            print(f\"  Problem {i+1}: Candidate {j+1} compiled but tests failed ({test_result['test_results']['failed']}/{test_result['test_results']['total']} failed)\")\n",
    "                else:\n",
    "                    # Persist data for failed extractions too\n",
    "                    no_code_result = {\n",
    "                        'build_passed': False,\n",
    "                        'test_passed': False,\n",
    "                        'build_stderr': 'No valid Ballerina code found in response',\n",
    "                        'test_stderr': 'No valid Ballerina code found in response',\n",
    "                        'test_results': {'passed': 0, 'failed': 0, 'total': 0}\n",
    "                    }\n",
    "                    self._persist_evaluation_data(\n",
    "                        metric_type=\"pass\",\n",
    "                        problem_idx=i,\n",
    "                        prompt=prompt,\n",
    "                        reference=test_case,  # Use test case as reference for debug\n",
    "                        candidate_idx=j,\n",
    "                        raw_response=cand,\n",
    "                        extracted_code=\"\",\n",
    "                        extracted_tests=test_case,\n",
    "                        result=no_code_result,\n",
    "                        k_value=k\n",
    "                    )\n",
    "\n",
    "            if any_passes:\n",
    "                passed_problems += 1\n",
    "\n",
    "        effective_total = total_problems - skipped_problems\n",
    "        print(f\"Pass@{k}: {passed_problems}/{effective_total} problems passed ({skipped_problems} skipped)\")\n",
    "        return passed_problems / effective_total if effective_total > 0 else 0.0\n",
    "\n",
    "    def generate_predictions(self, prompts: List[str], max_new_tokens: int = 1024, num_samples: int = 1) -> List[List[str]]:\n",
    "        \"\"\"Generate predictions for evaluation\"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"Generating predictions for prompt {i+1}/{len(prompts)}\")\n",
    "            samples = []\n",
    "            for _ in range(num_samples):\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "\n",
    "                inputs = self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        input_ids=inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=1.0,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                generated = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "                samples.append(generated.strip())\n",
    "\n",
    "            predictions.append(samples)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def load_evaluation_dataset(self, gist_url=None, max_samples=100):\n",
    "        \"\"\"Load the evaluation dataset\"\"\"\n",
    "        if gist_url is None:\n",
    "            gist_url = \"https://gist.githubusercontent.com/xlight05/32155e3065858b61b52adf940294c193/raw/f72488a75f2de40c818b729d3be641f30b659885/humaneval_bal.json\"\n",
    "\n",
    "        print(f\"Loading dataset from: {gist_url}\")\n",
    "        response = requests.get(gist_url)\n",
    "        json_data = response.text\n",
    "        data = json.loads(json_data)\n",
    "\n",
    "        # Create dataset and limit to max_samples\n",
    "        dataset = Dataset.from_list(data)\n",
    "        if len(dataset) > max_samples:\n",
    "            dataset = dataset.select(range(max_samples))\n",
    "\n",
    "        print(f\"Dataset loaded with {len(dataset)} samples\")\n",
    "        return dataset\n",
    "\n",
    "    def evaluate_dataset(self, eval_dataset, num_samples: int = 1, max_new_tokens: int = 1024) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive evaluation on a dataset\n",
    "        \n",
    "        Metrics calculated:\n",
    "        - pass@k: Percentage of problems where at least one of k candidates passes unit tests\n",
    "        - compile@k: Percentage of problems where at least one of k candidates compiles successfully\n",
    "        - BLEU, ROUGE, CodeBLEU, exact match: Code generation quality metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract prompts and test cases from dataset\n",
    "        prompts = []\n",
    "        test_cases = []\n",
    "\n",
    "        for item in eval_dataset:\n",
    "            prompts.append(item['prompt'])\n",
    "            test_cases.append(item['test'])\n",
    "\n",
    "        print(f\"Evaluating base model on {len(prompts)} examples...\")\n",
    "        \n",
    "        # Save evaluation metadata\n",
    "        self._save_evaluation_metadata(len(prompts), num_samples, max_new_tokens)\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = self.generate_predictions(prompts, max_new_tokens, num_samples)\n",
    "\n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'model_name': self.model_name,\n",
    "            'evaluation_type': 'base_model',\n",
    "            'num_samples': len(prompts),\n",
    "            'compilation_results': [],\n",
    "            'pass_at_1': 0.0,\n",
    "            'pass_at_5': 0.0 if num_samples >= 5 else None,\n",
    "            'compile_at_1': 0.0,\n",
    "            'compile_at_5': 0.0 if num_samples >= 5 else None,\n",
    "            'syntax_features': defaultdict(list)\n",
    "        }\n",
    "\n",
    "        for i, (test_case, preds) in enumerate(zip(test_cases, predictions)):\n",
    "            print(f\"Evaluating sample {i+1}/{len(test_cases)}\")\n",
    "\n",
    "            # Use the first prediction for most metrics, extract code from LLM response\n",
    "            pred = preds[0] if preds else \"\"\n",
    "            extracted_pred_code = exact_ballerina_main_code(pred) if pred else \"\"\n",
    "\n",
    "            # For this new approach, we don't have reference code to compare against\n",
    "            # Instead, we focus on compilation and test success metrics\n",
    "            # Skip BLEU, ROUGE, CodeBLEU, exact match, edit distance for now\n",
    "            # since we don't have reference implementations to compare against\n",
    "\n",
    "            # Syntax checking for all predictions\n",
    "            compilation_results = []\n",
    "            for pred in preds:\n",
    "                # Extract Ballerina code from LLM response\n",
    "                extracted_code = exact_ballerina_main_code(pred)\n",
    "                if extracted_code:  # Only check if we extracted valid code\n",
    "                    syntax_result = self.check_ballerina_syntax(extracted_code)\n",
    "                else:\n",
    "                    # No valid code found in response\n",
    "                    syntax_result = {\n",
    "                        'compiles': False,\n",
    "                        'error_message': 'No valid Ballerina code found in response',\n",
    "                        'has_function': False,\n",
    "                        'has_service': False,\n",
    "                        'has_import': False,\n",
    "                        'has_return': False\n",
    "                    }\n",
    "                compilation_results.append(syntax_result)\n",
    "\n",
    "                # Track syntax features\n",
    "                for feature, value in syntax_result.items():\n",
    "                    if feature != 'error_message':\n",
    "                        results['syntax_features'][feature].append(value)\n",
    "\n",
    "            results['compilation_results'].append(compilation_results)\n",
    "\n",
    "        # Calculate compile@k metrics (compilation success only)\n",
    "        print(f\"\\nCalculating Compile@k metrics (compilation success only)...\")\n",
    "        results['compile_at_1'] = self.calculate_compile_at_k(test_cases, predictions, prompts, k=1)\n",
    "        if num_samples >= 5:\n",
    "            results['compile_at_5'] = self.calculate_compile_at_k(test_cases, predictions, prompts, k=5)\n",
    "\n",
    "        # Calculate pass@k metrics (unit test success)\n",
    "        print(f\"\\nCalculating Pass@k metrics (unit test success)...\")\n",
    "        results['pass_at_1'] = self.calculate_pass_at_k(test_cases, predictions, prompts, k=1)\n",
    "        if num_samples >= 5:\n",
    "            results['pass_at_5'] = self.calculate_pass_at_k(test_cases, predictions, prompts, k=5)\n",
    "        \n",
    "\n",
    "        # Calculate aggregate statistics for compilation\n",
    "        results['compilation_success_rate'] = np.mean([cr[0]['compiles'] for cr in results['compilation_results']])\n",
    "\n",
    "        # Syntax feature statistics\n",
    "        for feature, values in results['syntax_features'].items():\n",
    "            if isinstance(values[0], bool):\n",
    "                results[f'avg_{feature}'] = np.mean(values)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def print_evaluation_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print a summary of evaluation results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BASE MODEL EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Model: {results['model_name']}\")\n",
    "        print(f\"Evaluation Type: {results['evaluation_type']}\")\n",
    "        print(f\"Number of Samples: {results['num_samples']}\")\n",
    "\n",
    "        print(f\"\\nCode Compilation & Testing:\")\n",
    "        print(f\"  Pass@1 (Unit Tests):  {results['pass_at_1']:.4f}\")\n",
    "        if results['pass_at_5'] is not None:\n",
    "            print(f\"  Pass@5 (Unit Tests):  {results['pass_at_5']:.4f}\")\n",
    "        print(f\"  Compile@1:            {results['compile_at_1']:.4f}\")\n",
    "        if results['compile_at_5'] is not None:\n",
    "            print(f\"  Compile@5:            {results['compile_at_5']:.4f}\")\n",
    "        print(f\"  Compilation Success:  {results['compilation_success_rate']:.4f}\")\n",
    "\n",
    "        print(f\"\\nSyntax Features:\")\n",
    "        for feature in ['has_function', 'has_service', 'has_import', 'has_return']:\n",
    "            if f'avg_{feature}' in results:\n",
    "                print(f\"  {feature.replace('_', ' ').title():15s}: {results[f'avg_{feature}']:.4f}\")\n",
    "\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    def run_sample_evaluation(self):\n",
    "        \"\"\"Run evaluation on sample prompts\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAMPLE PREDICTIONS FROM BASE MODEL\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        sample_prompts = [\n",
    "            \"Write a Ballerina function that calculates the sum of two numbers\",\n",
    "            \"Create a Ballerina HTTP service that responds with 'Hello World'\",\n",
    "            \"Write a Ballerina function that reads a file and returns its content\",\n",
    "            \"Create a Ballerina function to connect to a database and fetch user data\",\n",
    "            \"Write a Ballerina service that handles JSON requests and responses\"\n",
    "        ]\n",
    "\n",
    "        sample_predictions = self.generate_predictions(sample_prompts, max_new_tokens=300, num_samples=1)\n",
    "\n",
    "        for i, (prompt, preds) in enumerate(zip(sample_prompts, sample_predictions)):\n",
    "            print(f\"\\nðŸ“ Prompt {i+1}: {prompt}\")\n",
    "            print(f\"ðŸ¤– Generated Response:\\n{preds[0]}\")\n",
    "\n",
    "            # Extract and display the code\n",
    "            extracted_code = exact_ballerina_main_code(preds[0])\n",
    "            if extracted_code:\n",
    "                print(f\"ðŸ” Extracted Code:\\n{extracted_code}\")\n",
    "                # Check syntax on extracted code\n",
    "                syntax_result = self.check_ballerina_syntax(extracted_code)\n",
    "                print(f\"âœ… Compiles: {syntax_result['compiles']}\")\n",
    "                if not syntax_result['compiles'] and syntax_result['error_message']:\n",
    "                    print(f\"âŒ Error: {syntax_result['error_message']}\")\n",
    "            else:\n",
    "                print(\"ðŸ” No valid Ballerina code found in response\")\n",
    "                print(\"âŒ Error: Could not extract code from response\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download Ballerina .deb package\n",
    "ballerina_url = \"https://dist.ballerina.io/downloads/2201.12.7/ballerina-2201.12.7-swan-lake-linux-x64.deb\"\n",
    "deb_filename = \"ballerina-2201.12.7-swan-lake-linux-x64.deb\"\n",
    "\n",
    "print(\"Downloading Ballerina...\")\n",
    "urllib.request.urlretrieve(ballerina_url, deb_filename)\n",
    "print(f\"âœ… Downloaded {deb_filename}\")\n",
    "\n",
    "# Install the .deb package\n",
    "print(\"Installing Ballerina...\")\n",
    "try:\n",
    "    subprocess.run([\"dpkg\", \"-i\", deb_filename], check=True)\n",
    "    print(\"âœ… Ballerina installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Installation failed: {e}\")\n",
    "    print(\"Trying to fix dependencies...\")\n",
    "    subprocess.run([\"sudo\", \"apt-get\", \"-f\", \"install\"], check=True)\n",
    "\n",
    "# Test Ballerina version\n",
    "print(\"Testing Ballerina installation...\")\n",
    "try:\n",
    "    result = subprocess.run([\"bal\", \"-v\"], capture_output=True, text=True, check=True)\n",
    "    print(\"âœ… Ballerina version:\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"âŒ Failed to run 'bal -v': {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ 'bal' command not found. Installation may have failed.\")\n",
    "\n",
    "# Clean up downloaded file\n",
    "os.remove(deb_filename)\n",
    "print(f\"ðŸ§¹ Cleaned up {deb_filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "modelName = \"xlight05/base_test_4_grpo_16bit_vllm\"\n",
    "def main():\n",
    "    \"\"\"Main evaluation function\"\"\"\n",
    "    print(\"Starting Ballerina Base Model Evaluation\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = BallerinaBaseEvaluator(modelName)\n",
    "\n",
    "    # Load evaluation dataset (limit to 50 samples for faster evaluation)\n",
    "    eval_dataset = evaluator.load_evaluation_dataset(max_samples=1000)\n",
    "\n",
    "    # Run comprehensive evaluation\n",
    "    print(\"\\nRunning comprehensive evaluation...\")\n",
    "    evaluation_results = evaluator.evaluate_dataset(\n",
    "        eval_dataset,\n",
    "        num_samples=5,  # Generate 5 samples per prompt for pass@5 metric\n",
    "        max_new_tokens=1024\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    evaluator.print_evaluation_summary(evaluation_results)\n",
    "\n",
    "    # Save results\n",
    "    output_file = 'base_model_evaluation_results.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        results_for_json = {}\n",
    "        for key, value in evaluation_results.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                results_for_json[key] = value.tolist()\n",
    "            elif isinstance(value, (np.float64, np.float32)):\n",
    "                results_for_json[key] = float(value)\n",
    "            elif isinstance(value, (np.int64, np.int32)):\n",
    "                results_for_json[key] = int(value)\n",
    "            elif isinstance(value, defaultdict):\n",
    "                results_for_json[key] = dict(value)\n",
    "            else:\n",
    "                results_for_json[key] = value\n",
    "\n",
    "        json.dump(results_for_json, f, indent=2)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluation results saved to '{output_file}'\")\n",
    "\n",
    "    # # Run sample evaluation\n",
    "    # evaluator.run_sample_evaluation()\n",
    "\n",
    "    # print(\"\\nðŸŽ‰ Base model evaluation completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp base_model_evaluation_results.json debug_outputs/\n",
    "!zip -q -r debug_outputs.zip debug_outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T03:01:42.380384Z",
     "iopub.status.busy": "2025-08-16T03:01:42.380116Z",
     "iopub.status.idle": "2025-08-16T03:01:42.653691Z",
     "shell.execute_reply": "2025-08-16T03:01:42.652684Z",
     "shell.execute_reply.started": "2025-08-16T03:01:42.380361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, requests, pathlib\n",
    "\n",
    "def upload_and_notify(path):\n",
    "    with open(\"./debug_outputs.zip\", \"rb\") as f:\n",
    "        r = requests.post(\"https://temp.sh/upload\", files={\"file\": f})\n",
    "    print(r.text.strip())\n",
    "    url = r.text.strip() \n",
    "\n",
    "\n",
    "    # 2) Post to a request bin (webhook.site / RequestBin)\n",
    "    webhook = \"https://webhook.site/652f8f86-04e2-49d2-a6e0-4dc25e7490f0\"\n",
    "    if webhook:\n",
    "        try:\n",
    "            requests.post(webhook, json={\"file_url\": url, \"host\": \"transfer.sh\"}, timeout=10)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "    return url\n",
    "\n",
    "# use it:\n",
    "upload_and_notify(\"./debug_outputs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T02:58:53.283202Z",
     "iopub.status.busy": "2025-08-16T02:58:53.282468Z",
     "iopub.status.idle": "2025-08-16T02:58:53.360906Z",
     "shell.execute_reply": "2025-08-16T02:58:53.359649Z",
     "shell.execute_reply.started": "2025-08-16T02:58:53.283177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"runpodctl stop pod $RUNPOD_POD_ID\")\n",
    "os.system(\"runpodctl terminate pod $RUNPOD_POD_ID\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
