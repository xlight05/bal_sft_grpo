{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install transformers==4.51.3\n",
    "    !pip install --no-deps unsloth\n",
    "\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download Ballerina .deb package\n",
    "ballerina_url = \"https://dist.ballerina.io/downloads/2201.12.7/ballerina-2201.12.7-swan-lake-linux-x64.deb\"\n",
    "deb_filename = \"ballerina-2201.12.7-swan-lake-linux-x64.deb\"\n",
    "\n",
    "print(\"Downloading Ballerina...\")\n",
    "urllib.request.urlretrieve(ballerina_url, deb_filename)\n",
    "print(f\"✅ Downloaded {deb_filename}\")\n",
    "\n",
    "# Install the .deb package\n",
    "print(\"Installing Ballerina...\")\n",
    "try:\n",
    "    subprocess.run([\"dpkg\", \"-i\", deb_filename], check=True)\n",
    "    print(\"✅ Ballerina installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Installation failed: {e}\")\n",
    "    print(\"Trying to fix dependencies...\")\n",
    "    subprocess.run([\"sudo\", \"apt-get\", \"-f\", \"install\"], check=True)\n",
    "\n",
    "# Test Ballerina version\n",
    "print(\"Testing Ballerina installation...\")\n",
    "try:\n",
    "    result = subprocess.run([\"bal\", \"-v\"], capture_output=True, text=True, check=True)\n",
    "    print(\"✅ Ballerina version:\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Failed to run 'bal -v': {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ 'bal' command not found. Installation may have failed.\")\n",
    "\n",
    "# Clean up downloaded file\n",
    "os.remove(deb_filename)\n",
    "print(f\"🧹 Cleaned up {deb_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "WANDB_API_KEY = os.environ.get(\"wandb_api_key\")\n",
    "\n",
    "print(HF_TOKEN)\n",
    "print(WANDB_API_KEY)\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from typing import Optional, List\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset_url\": \"https://gist.githubusercontent.com/xlight05/860d56e432adbbcf5428aca45382c2d1/raw/ff442cbd0d3509a16940b28c4e200e554029d7de/combined.json\",  # Replace with actual Gist URL\n",
    "    # \"model_name\": \"xlight05/bal_coder_full_16bit_vllm\",\n",
    "    \"model_name\" : \"unsloth/Qwen2.5-Coder-7B-Instruct\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"lora_rank\": 16,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"max_steps\": 10, # change\n",
    "    \"save_steps\": 5, # change\n",
    "    \"num_generations\": 2,\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"num_train_epochs\" : 1,\n",
    "    \"run_name\": \"base_test_1_\"\n",
    "}\n",
    "\n",
    "def generate_model_name(training_type: str, format_type: str, bits: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a model name based on config and training parameters.\n",
    "    \n",
    "    Args:\n",
    "        training_type: \"sft\" or \"grpo\"\n",
    "        format_type: \"vllm\" or \"gguf\"\n",
    "        bits: \"4bit\", \"8bit\", \"16bit\", or None for GGUF\n",
    "    \n",
    "    Returns:\n",
    "        Formatted model name string\n",
    "    \"\"\"\n",
    "    base_name = CONFIG[\"run_name\"]\n",
    "    \n",
    "    # Build the suffix\n",
    "    suffix_parts = [training_type]\n",
    "    \n",
    "    if bits:\n",
    "        suffix_parts.append(bits)\n",
    "    \n",
    "    suffix_parts.append(format_type)\n",
    "    \n",
    "    suffix = \"_\".join(suffix_parts)\n",
    "    \n",
    "    # Combine with base name, ensuring proper formatting\n",
    "    model_name = f\"xlight05/{base_name}{suffix}\"\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "# Helper function to update Gist URL\n",
    "def set_dataset_url(gist_url: str):\n",
    "    \"\"\"Update the dataset URL in CONFIG\"\"\"\n",
    "    CONFIG[\"dataset_url\"] = gist_url\n",
    "    print(f\"Dataset URL updated to: {gist_url}\")\n",
    "\n",
    "print(\"✅ Configuration loaded!\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Dataset URL: {CONFIG['dataset_url']}\")\n",
    "print(f\"Run name: {CONFIG['run_name']}\")\n",
    "\n",
    "# # Test the model name generation\n",
    "# print(\"\\nExample model names:\")\n",
    "# print(f\"SFT 16bit VLLM: {generate_model_name('sft', 'vllm', '16bit')}\")\n",
    "# print(f\"SFT 8bit VLLM: {generate_model_name('sft', 'vllm', '8bit')}\")\n",
    "# print(f\"SFT GGUF: {generate_model_name('sft', 'gguf')}\")\n",
    "# print(f\"GRPO 16bit VLLM: {generate_model_name('grpo', 'vllm', '16bit')}\")\n",
    "# print(f\"GRPO GGUF: {generate_model_name('grpo', 'gguf')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "lora_rank = 16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16*2,\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "TDD_SYSTEM_PROMPT = \"\"\"You are a pragmatic Ballerina programmer who enjoys test driven development. Given the following question, write a Ballerina function to complete the task and then write the the unit tests to validate the function.\n",
    "\n",
    "1. Make the code simple and easy to understand.\n",
    "2. Try to limit library usage to the standard library. Be careful with your types, and try to limit yourself to the basic built in types and standard library functions.\n",
    "3. Before you start writing the function you can think through how to solve the problem and perform reasoning in the comments above the function.\n",
    "4. Then write unit tests for the function you defined. Make sure to write at least 4 assertions to test the function. The tests should be a simple.\n",
    "\n",
    "Strictly follow the following output format for each response: Make sure to include code inside <CODE> and <TESTS> blocks.\n",
    "\n",
    "# Overview\n",
    "Brief overview about the solution.\n",
    "\n",
    "<CODE>\n",
    "```ballerina\n",
    "// Reasoning goes here\n",
    "// and can be multi-line\n",
    "function add(int a, int b) returns int {\n",
    "    return a + b;\n",
    "}\n",
    "```\n",
    "</CODE>\n",
    "\n",
    "<TESTS>\n",
    "```ballerina\n",
    "import ballerina/test;\n",
    "\n",
    "@test:Config { }\n",
    "function testAssertEquals() {\n",
    "    int addResult = add(40, 2);\n",
    "    test:assertEquals(addResult, 42);\n",
    "\n",
    "    addResult = add(0, 0);\n",
    "    test:assertEquals(addResult, 0);\n",
    "\n",
    "    addResult = add(-1, 1);\n",
    "    test:assertEquals(addResult, 0);\n",
    "\n",
    "    addResult = add(-5, -5);\n",
    "    test:assertEquals(addResult, -10);\n",
    "}\n",
    "```\n",
    "</TESTS>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "BBE_SYSTEM_PROMPT = \"\"\"\"You are a pragmatic Ballerina programmer. Given the following question, write the code to complete the task.\n",
    "Strictly follow the following output format for each response: Make sure to include code inside <CODE> blocks.\n",
    "\n",
    "<CODE>\n",
    "```ballerina\n",
    "// Reasoning goes here\n",
    "// and can be multi-line\n",
    "function add(int a, int b) returns int {\n",
    "    return a + b;\n",
    "}\n",
    "```\n",
    "</CODE>\n",
    "\"\"\"\n",
    "\n",
    "# Define the gist URLs with their corresponding system prompts\n",
    "gist_configs = [\n",
    "    {\n",
    "        \"url\": \"https://gist.githubusercontent.com/xlight05/f8e1e94c7b65c2e34dac70bb27f04f0b/raw/2705c986f2e57c44085360ec9bd0258b23347ce6/bbe_train.json\",\n",
    "        \"system_prompt\": BBE_SYSTEM_PROMPT\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://gist.githubusercontent.com/xlight05/67fcc85b8b549b7919772bc43e9c2fc5/raw/e034ae3f29adb58fb10b477d6bfaa1f4575340ba/tdd_train.json\",\n",
    "        \"system_prompt\": TDD_SYSTEM_PROMPT\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load and combine data from all gists\n",
    "combined_data = []\n",
    "for config in gist_configs:\n",
    "    # Download the gist content\n",
    "    response = requests.get(config[\"url\"])\n",
    "    json_data = response.text\n",
    "    \n",
    "    # Load the JSON data\n",
    "    data = json.loads(json_data)\n",
    "    \n",
    "    # Add system prompt information to each item\n",
    "    for item in data:\n",
    "        item[\"system_prompt\"] = config[\"system_prompt\"]\n",
    "    \n",
    "    combined_data.extend(data)\n",
    "\n",
    "# Create a dataset from the combined list of dictionaries\n",
    "dataset = Dataset.from_list(combined_data)\n",
    "\n",
    "# prompt: dataset length\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "dataset[0]['answer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to create the 'conversations' field\n",
    "def create_conversations_field(examples):\n",
    "    \"\"\"Formats an example into a conversational structure.\"\"\"\n",
    "    # examples is a dictionary where keys are column names and values are lists when batched=True\n",
    "    prompts = examples['prompt']\n",
    "    main_codes = examples['answer']\n",
    "    system_prompts = examples['system_prompt']\n",
    "\n",
    "    conversations_batch = []\n",
    "    for prompt, main_code, system_prompt in zip(prompts, main_codes, system_prompts):\n",
    "        conversations = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": main_code},\n",
    "        ]\n",
    "        conversations_batch.append(conversations)\n",
    "\n",
    "    # Return a dictionary with the new 'conversations' column\n",
    "    return {\"conversations\": conversations_batch}\n",
    "\n",
    "# Function to apply the chat template to the conversations field\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Applies the chat template to the 'conversations' field.\"\"\"\n",
    "    # examples is a dictionary where keys are column names and values are lists when batched=True\n",
    "    convos = examples[\"conversations\"]\n",
    "    # Apply chat template to each list of conversations in the batch\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Apply the first mapping to create the 'conversations' field (batched=True is more efficient)\n",
    "dataset_with_conversations = dataset.map(create_conversations_field, batched=True, )\n",
    "\n",
    "# Apply the second mapping to create the 'text' field from 'conversations' (batched=True is also efficient here)\n",
    "dataset = dataset_with_conversations.map(formatting_prompts_func, batched=True, )\n",
    "\n",
    "# Now the dataset has both \"conversations\" and \"text\" fields\n",
    "print(dataset[0])\n",
    "\n",
    "# Now you can access the data in the dataset\n",
    "print(dataset[0]) # Print the first example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
    "\n",
    "The trainer includes our **gradient accumulation bug fix**. Read more about it here: [Blog post](https://unsloth.ai/blog/gradient)\n",
    "\"\"\"\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 4,\n",
    "    packing = True,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 50,\n",
    "        num_train_epochs = CONFIG['num_train_epochs'],\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\"\"\"We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs.\"\"\"\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "\"\"\"We verify masking is actually done:\"\"\"\n",
    "\n",
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n",
    "\n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Train using SFT\"\"\"\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "\n",
    "\n",
    "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why.\n",
    "\"\"\"\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!\n",
    "\"\"\"\n",
    "\n",
    "# model.save_pretrained(\"bal_coder_lora2\")  # Local saving\n",
    "# tokenizer.save_pretrained(\"bal_coder_lora2\")\n",
    "# model.push_to_hub(\"xlight05/bal_coder_lora2_int\", token = HF_TOKEN) # Online saving\n",
    "# tokenizer.push_to_hub(\"xlight05/bal_coder_lora2_int\", token = HF_TOKEN) # Online saving\n",
    "\n",
    "\"\"\"### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n",
    "\"\"\"\n",
    "\n",
    "# Generate model names based on config\n",
    "sft_4bit_vllm_name = generate_model_name('sft', 'vllm', '4bit')\n",
    "sft_8bit_vllm_name = generate_model_name('sft', 'vllm', '8bit')\n",
    "sft_16bit_vllm_name = generate_model_name('sft', 'vllm', '16bit')\n",
    "sft_gguf_name = generate_model_name('sft', 'gguf')\n",
    "\n",
    "# print(f\"Generated model names:\")\n",
    "# print(f\"4bit VLLM: {sft_4bit_vllm_name}\")\n",
    "# print(f\"8bit VLLM: {sft_8bit_vllm_name}\")\n",
    "# print(f\"16bit VLLM: {sft_16bit_vllm_name}\")\n",
    "# print(f\"GGUF: {sft_gguf_name}\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(sft_4bit_vllm_name, tokenizer, save_method = \"merged_4bit\", token = HF_TOKEN)\n",
    "\n",
    "# # Merge to 16bit\n",
    "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "# if False: model.push_to_hub_merged(sft_16bit_vllm_name, tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
    "\n",
    "# # Just LoRA adapters\n",
    "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")\n",
    "\n",
    "if True: model.push_to_hub_merged(sft_8bit_vllm_name, tokenizer, save_method = \"merged_8bit\", token = HF_TOKEN)\n",
    "\n",
    "if True: model.push_to_hub_merged(sft_16bit_vllm_name, tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n",
    "\n",
    "\n",
    "\"\"\"### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "\"\"\"\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "# if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if True: model.push_to_hub_gguf(sft_gguf_name, tokenizer, token = HF_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the code block delimiters\n",
    "code_start = \"<CODE>\"\n",
    "code_end = \"</CODE>\"\n",
    "test_start = \"<TESTS>\"\n",
    "test_end = \"</TESTS>\"\n",
    "import difflib\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "class BallerinaManager:\n",
    "    def __init__(self, project_path: str = \".\"):\n",
    "        self.project_path = project_path\n",
    "    \n",
    "    def get_build_status(self) -> Dict[str, any]:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"bal\", \"build\", \"--offline\"],\n",
    "                cwd=self.project_path,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": result.returncode == 0,\n",
    "                \"return_code\": result.returncode,\n",
    "                \"stdout\": result.stdout,\n",
    "                \"stderr\": result.stderr,\n",
    "                \"compilation_errors\": self._extract_compilation_errors(result.stdout + result.stderr)\n",
    "            }\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"Build process timed out\",\n",
    "                \"compilation_errors\": [\"Build process timed out after 60 seconds\"]\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"bal command not found\",\n",
    "                \"compilation_errors\": [\"Ballerina CLI not found. Please ensure Ballerina is installed and in PATH\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": str(e),\n",
    "                \"compilation_errors\": [f\"Unexpected error: {str(e)}\"]\n",
    "            }\n",
    "    \n",
    "    def get_test_status(self) -> Dict[str, any]:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"bal\", \"test\", \"--offline\"],\n",
    "                cwd=self.project_path,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=120\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": result.returncode == 0,\n",
    "                \"return_code\": result.returncode,\n",
    "                \"stdout\": result.stdout,\n",
    "                \"stderr\": result.stderr,\n",
    "                \"test_results\": self._extract_test_results(result.stdout + result.stderr),\n",
    "                \"compilation_errors\": self._extract_compilation_errors(result.stdout + result.stderr)\n",
    "            }\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"Test process timed out\",\n",
    "                \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                \"compilation_errors\": [\"Test process timed out after 120 seconds\"]\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": \"bal command not found\",\n",
    "                \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                \"compilation_errors\": [\"Ballerina CLI not found. Please ensure Ballerina is installed and in PATH\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"return_code\": -1,\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": str(e),\n",
    "                \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                \"compilation_errors\": [f\"Unexpected error: {str(e)}\"]\n",
    "            }\n",
    "    \n",
    "    def _extract_compilation_errors(self, output: str) -> List[str]:\n",
    "        errors = []\n",
    "        lines = output.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(keyword in line.lower() for keyword in ['error:', 'compilation error', 'build failed']):\n",
    "                errors.append(line)\n",
    "            elif line.startswith('ERROR') or 'error occurred' in line.lower():\n",
    "                errors.append(line)\n",
    "        \n",
    "        return errors\n",
    "    \n",
    "    def _extract_test_results(self, output: str) -> Dict[str, int]:\n",
    "        results = {\"passed\": 0, \"failed\": 0, \"total\": 0}\n",
    "        lines = output.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Look for Ballerina test output format: \"X passing\", \"Y failing\", \"Z skipped\"\n",
    "            if 'passing' in line:\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if part == 'passing' and i > 0:\n",
    "                            results[\"passed\"] = int(parts[i-1])\n",
    "                            break\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "            elif 'failing' in line:\n",
    "                try:\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if part == 'failing' and i > 0:\n",
    "                            results[\"failed\"] = int(parts[i-1])\n",
    "                            break\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "        \n",
    "        results[\"total\"] = results[\"passed\"] + results[\"failed\"]\n",
    "        return results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from uuid import uuid4\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "Define functions for setting up and testing Ballerina projects.\n",
    "\"\"\"\n",
    "\n",
    "def create_ballerina_toml(package_name: str) -> str:\n",
    "    return f\"\"\"[package]\n",
    "org = \"test\"\n",
    "name = \"test_project\"\n",
    "version = \"0.1.0\"\n",
    "distribution = \"2201.12.7\"\n",
    "\n",
    "[build-options]\n",
    "observabilityIncluded = false\n",
    "\"\"\"\n",
    "\n",
    "def create_main_bal(main_content: str) -> str:\n",
    "    return f\"\"\"{main_content}\"\"\"\n",
    "\n",
    "def create_test_bal(test_content: str) -> str:\n",
    "    return f\"\"\"{test_content}\"\"\"\n",
    "\n",
    "def setup_build_ballerina(main_content: str, test_content: str) -> dict:\n",
    "    \"\"\"Set up temporary Ballerina project and run build with error handling\"\"\"\n",
    "    try:\n",
    "        # Create temporary directory with random UUID suffix\n",
    "        package_name = f\"test-project-{str(uuid4())[:8]}\"\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            project_dir = Path(temp_dir) / package_name\n",
    "            project_dir.mkdir()\n",
    "            tests_dir = project_dir / \"tests\"\n",
    "            tests_dir.mkdir()\n",
    "\n",
    "            # Write project files\n",
    "            (project_dir / \"Ballerina.toml\").write_text(create_ballerina_toml(package_name))\n",
    "            (project_dir / \"main.bal\").write_text(create_main_bal(main_content))\n",
    "            (tests_dir / \"test.bal\").write_text(create_test_bal(test_content))\n",
    "\n",
    "            # Use BallerinaManager to get build status\n",
    "            ballerina_manager = BallerinaManager(str(project_dir))\n",
    "            build_result = ballerina_manager.get_build_status()\n",
    "\n",
    "            return {\n",
    "                \"build_passed\": build_result[\"success\"],\n",
    "                \"build_stderr\": build_result[\"stderr\"],\n",
    "                \"compilation_errors\": build_result[\"compilation_errors\"],\n",
    "                \"package_name\": package_name\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Ballerina project: {e}\")\n",
    "        return {\n",
    "            \"build_passed\": False,\n",
    "            \"build_stderr\": f\"Project setup error: {e}\",\n",
    "            \"compilation_errors\": [f\"Project setup error: {e}\"],\n",
    "            \"package_name\": \"unknown\"\n",
    "        }\n",
    "\n",
    "def setup_build_test_ballerina(main_content: str, test_content: str) -> dict:\n",
    "    \"\"\"Set up temporary Ballerina project with tests and run build and test with error handling\"\"\"\n",
    "    try:\n",
    "        # Create temporary directory with random UUID suffix\n",
    "        package_name = f\"test-project-{str(uuid4())[:8]}\"\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            project_dir = Path(temp_dir) / package_name\n",
    "            project_dir.mkdir()\n",
    "            tests_dir = project_dir / \"tests\"\n",
    "            tests_dir.mkdir()\n",
    "\n",
    "            # Write project files\n",
    "            (project_dir / \"Ballerina.toml\").write_text(create_ballerina_toml(package_name))\n",
    "            (project_dir / \"main.bal\").write_text(create_main_bal(main_content))\n",
    "            (tests_dir / \"test.bal\").write_text(create_test_bal(test_content))\n",
    "\n",
    "            # Use BallerinaManager to get build and test status\n",
    "            ballerina_manager = BallerinaManager(str(project_dir))\n",
    "            \n",
    "            # Get build status first\n",
    "            build_result = ballerina_manager.get_build_status()\n",
    "            \n",
    "            # Get test status only if build succeeds\n",
    "            if build_result[\"success\"]:\n",
    "                test_result = ballerina_manager.get_test_status()\n",
    "            else:\n",
    "                test_result = {\n",
    "                    \"success\": False,\n",
    "                    \"stdout\": \"\",\n",
    "                    \"stderr\": \"Build failed, skipping tests\",\n",
    "                    \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "                    \"compilation_errors\": []\n",
    "                }\n",
    "\n",
    "            return {\n",
    "                \"build_passed\": build_result[\"success\"],\n",
    "                \"build_stderr\": build_result[\"stderr\"],\n",
    "                \"build_compilation_errors\": build_result[\"compilation_errors\"],\n",
    "                \"test_passed\": test_result[\"success\"],\n",
    "                \"test_stderr\": test_result[\"stderr\"],\n",
    "                \"test_results\": test_result[\"test_results\"],\n",
    "                \"test_compilation_errors\": test_result[\"compilation_errors\"],\n",
    "                \"package_name\": package_name\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Ballerina project with tests: {e}\")\n",
    "        return {\n",
    "            \"build_passed\": False,\n",
    "            \"build_stderr\": f\"Project setup error: {e}\",\n",
    "            \"build_compilation_errors\": [f\"Project setup error: {e}\"],\n",
    "            \"test_passed\": False,\n",
    "            \"test_stderr\": f\"Project setup error: {e}\",\n",
    "            \"test_results\": {\"passed\": 0, \"failed\": 0, \"total\": 0},\n",
    "            \"test_compilation_errors\": [f\"Project setup error: {e}\"],\n",
    "            \"package_name\": \"unknown\"\n",
    "        }\n",
    "\n",
    "print(\"✅ Ballerina project setup functions defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_ballerina_code(response: str) -> str:\n",
    "    \"\"\"Extract Ballerina code from response - extracts content inside ```ballerina blocks within <CODE> tags\"\"\"\n",
    "    # Extract everything between <CODE> and </CODE>\n",
    "    pattern = rf\"{re.escape(code_start)}(.*?){re.escape(code_end)}\"\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        # Now extract content from ```ballerina code block\n",
    "        ballerina_pattern = r\"```ballerina\\s*(.*?)\\s*```\"\n",
    "        ballerina_match = re.search(ballerina_pattern, content, re.DOTALL)\n",
    "        if ballerina_match:\n",
    "            return ballerina_match.group(1).strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_ballerina_tests(response: str) -> str:\n",
    "    \"\"\"Extract Ballerina tests from response - extracts content inside ```ballerina blocks within <TESTS> tags\"\"\"\n",
    "    # Extract everything between <TESTS> and </TESTS>\n",
    "    pattern = rf\"{re.escape(test_start)}(.*?){re.escape(test_end)}\"\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        # Now extract content from ```ballerina code block\n",
    "        ballerina_pattern = r\"```ballerina\\s*(.*?)\\s*```\"\n",
    "        ballerina_match = re.search(ballerina_pattern, content, re.DOTALL)\n",
    "        if ballerina_match:\n",
    "            return ballerina_match.group(1).strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def exact_ballerina_main_code(content: str) -> str:\n",
    "    \"\"\"Extract main Ballerina code content\"\"\"\n",
    "    return extract_ballerina_code(content)\n",
    "\n",
    "def exact_ballerina_test_code(content: str) -> str:\n",
    "    \"\"\"Extract test Ballerina code content\"\"\"\n",
    "    return extract_ballerina_tests(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def reward_build_test_content(content: str) -> float:\n",
    "    \"\"\"Reward for code with passing tests (higher reward) - content only version\"\"\"\n",
    "    code = exact_ballerina_main_code(content)\n",
    "    tests = exact_ballerina_test_code(content)\n",
    "\n",
    "    if not (code and tests):\n",
    "        return 0.0\n",
    "    if \"function \" not in code:\n",
    "        return 0.0\n",
    "    try:\n",
    "        results = setup_build_test_ballerina(code, tests)\n",
    "        # print(results)\n",
    "        if results[\"build_passed\"] is False:\n",
    "            diags = results.get(\"build_compilation_errors\", 0)\n",
    "            # print(f\"Compilation errors: {diags}\")\n",
    "            diag_count = len(diags) - 1\n",
    "            if diag_count == 1:\n",
    "                return 1.5\n",
    "            elif diag_count == 2:\n",
    "                return 1.0\n",
    "            elif diag_count == 3:\n",
    "                return 0.5\n",
    "            else:  # more than 3 diagnostics\n",
    "                return 0.0\n",
    "        else:\n",
    "            score = 3.0\n",
    "            testScore =  min(6.0, 1.5 * results[\"test_results\"][\"passed\"]) if results[\"test_passed\"] else 0.0\n",
    "            return score + testScore\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def reward_code_structure_content(content: str) -> float:\n",
    "    \"\"\"Reward for having proper structure with code and test blocks - content only version\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Check for code block (0.5 points)\n",
    "    code = exact_ballerina_main_code(content)\n",
    "    if code:\n",
    "        score += 0.5\n",
    "        \n",
    "        # Check if code has at least one function (0.5 points)\n",
    "        if \"function \" in code:\n",
    "            score += 0.5\n",
    "\n",
    "    # Check for test block (0.5 points)\n",
    "    tests = exact_ballerina_test_code(content)\n",
    "    if tests:\n",
    "        score += 0.5\n",
    "        \n",
    "        # Check for test import (0.1 points)\n",
    "        if \"import ballerina/test\" in tests:\n",
    "            score += 0.1\n",
    "        \n",
    "        # Check for test config (0.1 points)\n",
    "        if \"@test:Config\" in tests:\n",
    "            score += 0.1\n",
    "        \n",
    "        # Check for assertions (0.2 points each)\n",
    "        assert_count = tests.count(\"test:assert\")\n",
    "        score += min(assert_count * 0.2, 1.0)  # Cap at 1.0 for assertions\n",
    "\n",
    "    return score\n",
    "\n",
    "# # Wrapper functions that maintain the original interface for GRPO\n",
    "# def reward_build(completions, **kwargs) -> list[float]:\n",
    "#     \"\"\"Wrapper for build reward function\"\"\"\n",
    "#     return [reward_build_content(completion[0][\"content\"]) for completion in completions]\n",
    "\n",
    "def reward_build_test(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Wrapper for test reward function\"\"\"\n",
    "    return [reward_build_test_content(completion[0][\"content\"]) for completion in completions]\n",
    "\n",
    "def reward_code_structure(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Wrapper for code structure reward function\"\"\"\n",
    "    return [reward_code_structure_content(completion[0][\"content\"]) for completion in completions]\n",
    "\n",
    "\n",
    "print(\"✅ All reward functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset from Gist URL or use sample data.\n",
    "\"\"\"\n",
    "\n",
    "def load_gist_dataset(gist_url: str):\n",
    "    \"\"\"Load dataset from Gist URL containing JSON array\"\"\"\n",
    "    print(f\"Fetching dataset from: {gist_url}\")\n",
    "    response = requests.get(gist_url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse JSON array\n",
    "    dataset_json = response.json()\n",
    "\n",
    "    if not isinstance(dataset_json, list):\n",
    "        raise ValueError(\"Expected JSON array format\")\n",
    "\n",
    "    # Convert to expected format\n",
    "    formatted_samples = []\n",
    "    for item in dataset_json:\n",
    "        if not isinstance(item, dict) or \"prompt\" not in item:\n",
    "            print(f\"Skipping invalid item: {item}\")\n",
    "            continue\n",
    "\n",
    "        formatted_samples.append({\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": TDD_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": item[\"prompt\"]},\n",
    "            ],\n",
    "            \"response\": \"\"  # Will be generated during training\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(formatted_samples)} samples from Gist\")\n",
    "    return formatted_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Loading dataset from: {CONFIG['dataset_url']}\")\n",
    "dataset_samples = load_gist_dataset(CONFIG[\"dataset_url\"])\n",
    "\n",
    "# Convert to format expected by trainer\n",
    "from datasets import Dataset\n",
    "dataset_dict = {\n",
    "    \"prompt\": [sample[\"prompt\"] for sample in dataset_samples],\n",
    "    \"response\": [sample[\"response\"] for sample in dataset_samples]\n",
    "}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "print(f\"✅ Dataset created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the chat template with a sample prompt.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing chat template:\")\n",
    "test_messages = dataset[0][\"prompt\"]\n",
    "formatted = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(formatted)\n",
    "print(\"-\" * 50)\n",
    "print(\"✅ Chat template working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure GRPO training parameters and create trainer.\n",
    "\"\"\"\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# VLLM sampling parameters\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p=0.1,\n",
    "    top_p=1.0,\n",
    "    top_k=-1,\n",
    "    seed=3407,\n",
    "    stop=[tokenizer.eos_token],\n",
    "    include_stop_str_in_output=True,\n",
    ")\n",
    "\n",
    "# GRPO training configuration\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params=vllm_sampling_params,\n",
    "    temperature=1.0,\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    num_generations=CONFIG[\"num_generations\"],\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=CONFIG[\"max_seq_length\"] - 512,\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"rust_grpo_outputs\",\n",
    ")\n",
    "\n",
    "print(\"Setting up GRPO trainer...\")\n",
    "\n",
    "# Create trainer with all reward functions matching original\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        reward_code_structure,\n",
    "        reward_build_test,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"✅ GRPO trainer configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start the GRPO training process.\n",
    "WARNING: This may take a long time as each generation runs bal tools!\n",
    "\"\"\"\n",
    "\n",
    "print(\"🚀 Starting GRPO training...\")\n",
    "print(\"Note: This may take some time as each generation runs bal tools...\")\n",
    "print(\"Watch the reward values - they should increase over time!\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the trained model with inference.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing trained model...\")\n",
    "test_prompt = [\n",
    "    {\"role\": \"system\", \"content\": TDD_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"Write a function that calculates the factorial of a number.\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    test_prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "print(\"Generated response:\")\n",
    "print(\"-\" * 50)\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1024,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"✅ Inference test completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if True: model.push_to_hub_merged(generate_model_name('grpo', 'vllm', '16bit'), tokenizer, save_method = \"merged_16bit\", token = HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if True: model.push_to_hub_gguf(generate_model_name('grpo', 'gguf'), tokenizer, token = HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"runpodctl stop pod $RUNPOD_POD_ID\")\n",
    "os.system(\"runpodctl terminate pod $RUNPOD_POD_ID\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
